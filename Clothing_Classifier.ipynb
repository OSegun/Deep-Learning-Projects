{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlM+Rvs3kpxX3gfBegp9nP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OSegun/Deep-Learning-Projects/blob/main/Clothing_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fashion Forward is a new AI-based e-commerce clothing retailer.\n",
        "They want to use image classification to automatically categorize new product listings, making it easier for customers to find what they're looking for. It will also assist in inventory management by quickly sorting items.\n",
        "\n",
        "As a data scientist tasked with implementing a garment classifier, your primary objective is to develop a machine learning model capable of accurately categorizing images of clothing items into distinct garment types such as shirts, trousers, shoes, etc.\n"
      ],
      "metadata": {
        "id": "iODnTtC1iw93"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_N3oyNdivbM",
        "outputId": "39d020cb-fa49-4fcb-9c83-50d5cca2f2a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/890.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m880.6/890.6 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m880.6/890.6 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.6/890.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics --q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchmetrics import Accuracy, Precision, Recall\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import transforms"
      ],
      "metadata": {
        "id": "p2pRDCEui4h4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5xmOVmkjSbF",
        "outputId": "01819715-18c9-4f3f-bb1a-f867af7b8209"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 14.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 305kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.62MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 5.18MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = train_data.classes\n",
        "num_classes = len(classes)"
      ],
      "metadata": {
        "id": "8tUQvHsWj0V3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The Fashions to predicts are:\\n \", \", \".join(classes))\n",
        "print(\"The number of classes to predict is:\", num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdca335DlHnP",
        "outputId": "a791089e-8c67-44cb-a3f0-b3a11c311c11"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Fashions to predicts are:\n",
            "  T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
            "The number of classes to predict is: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = train_data[0][0].shape[1]\n",
        "image_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFXX8wCmlI1n",
        "outputId": "47dd50e1-e203-449c-a720-0bc4d5229ec3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, num_classes, image_size):\n",
        "    super(ImageClassifier, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.layer = nn.Linear(16 *(image_size//2)**2, num_classes)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.layer(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "EQPDjLGjqgZW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=10,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "UTPh4e4UtbkF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(optimizer, model, epochs):\n",
        "  processed = 0\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0\n",
        "    processed = 0\n",
        "    for feature, label in train_dataloader:\n",
        "      optimizer.zero_grad()\n",
        "      output = model(feature)\n",
        "      loss = criterion(output, label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "      processed += len(label)\n",
        "    print(f\"Epoch {epoch}, Loss: {running_loss/processed}\")\n",
        "  train_loss = running_loss /len(train_dataloader)"
      ],
      "metadata": {
        "id": "PX9LTZMquCEk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ImageClassifier(num_classes=num_classes, image_size=image_size)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train_model(\n",
        "    optimizer=optimizer,\n",
        "    model=model,\n",
        "    epochs=5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t81WCO7Avjws",
        "outputId": "fad4e246-856c-4800-a22c-0086c5b86bc7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.04089180020809872\n",
            "Epoch 1, Loss: 0.033682949545248024\n",
            "Epoch 2, Loss: 0.03177553457958505\n",
            "Epoch 3, Loss: 0.030830264964715266\n",
            "Epoch 4, Loss: 0.030341342069184127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=10,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "LicdrBorwB0k"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
        "precision = Precision(task=\"multiclass\", num_classes=num_classes, average=None)\n",
        "recall = Recall(task=\"multiclass\", num_classes=num_classes, average=None)"
      ],
      "metadata": {
        "id": "Tk7k_3iPyCq5"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "predicted = []\n",
        "for i, (feature, label) in enumerate(test_dataloader):\n",
        "  output = model.forward(feature.reshape(-1,1,image_size, image_size))\n",
        "  pred = torch.argmax(output, dim=-1)\n",
        "  predicted.extend(pred.tolist())\n",
        "  accuracy(pred,label)\n",
        "  precision(pred,label)\n",
        "  recall(pred, label)"
      ],
      "metadata": {
        "id": "HtnKjlqeyeCM"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_val = accuracy.compute().item()\n",
        "precision_val = precision.compute().tolist()\n",
        "recall_val = recall.compute().tolist()\n",
        "print(\"Accuracy: \", accuracy_val)\n",
        "print(\"Precision per class: \", precision_val)\n",
        "print(\"Recall per class: \", recall_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VTzUQoTz14y",
        "outputId": "5440e7f9-0dd3-415e-f555-c8a4646952be"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.8794999718666077\n",
            "Precision per class:  [0.7798408269882202, 0.9751737713813782, 0.8130939602851868, 0.8793274164199829, 0.7558348178863525, 0.9746450185775757, 0.739847719669342, 0.9572301506996155, 0.9817997813224792, 0.9330143332481384]\n",
            "Recall per class:  [0.8820000290870667, 0.9819999933242798, 0.7699999809265137, 0.8889999985694885, 0.8420000076293945, 0.9610000252723694, 0.5830000042915344, 0.9399999976158142, 0.9710000157356262, 0.9750000238418579]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JShnab0q0-_V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}